# MLX LLM Server Configuration Example
# Copy this file to .env and adjust the values

# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=8000  # Change to 443 for HTTPS
SERVER_WORKERS=1  # Keep at 1 for MLX

# SSL/TLS Configuration (optional - for HTTPS)
# Leave empty for HTTP development server
SSL_CERT=
SSL_KEY=

# Domain Configuration (optional - for production)
PRIMARY_DOMAIN=localhost
TAILSCALE_DOMAIN=
ALLOWED_ORIGINS=["http://localhost:3000","http://localhost:8000","http://127.0.0.1:8000"]

# Model Configuration
# You can use local paths or HuggingFace model IDs
MODELS_DIR=./models
DEFAULT_MODEL=mlx-community/Llama-3.2-3B-Instruct-4bit
MAX_MODEL_MEMORY_GB=24  # Adjust based on your system RAM

# API Configuration
API_PREFIX=/api/v1
MAX_TOKENS_DEFAULT=2048
MAX_TOKENS_LIMIT=32768

# Redis Configuration (optional - for session persistence)
# Leave as-is for in-memory sessions, or set up Redis for persistence
REDIS_URL=redis://localhost:6379/0
SESSION_TTL_HOURS=24

# Rate Limiting
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000

# Authentication
ENABLE_AUTH=true
# Generate keys with: ./generate_api_keys.py
# Format: ["key1", "key2"] for JSON array
API_KEYS=["your-api-key-here"]
# JWT secret will be auto-generated if not provided
JWT_SECRET=
JWT_ALGORITHM=HS256

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090

# Advanced Configuration (usually not needed)
# Voice API Configuration (for future voice processing split)
VOICE_API_HOST=
VOICE_SERVICES=[]